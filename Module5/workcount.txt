
WordCount Spark Example:

Upload a large file into HDFS

wget http://www.gutenberg.org/files/5000/5000-8.txt

hdfs dfs -mkdir /user/sshuser
hdfs dfs -copyFromLocal 5000-8.txt /user/sshuser

Start pyspark interactive session

pyspark

Initialize the Spark Context

text_file = sc.textFile("/user/sshuser/derby.log")

Create flatmap variable

counts = text_file.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

Save it

counts.saveAsTextFile("/user/sshuser/output")

HomeWork:

1. Take a large text file 10 GB, and run the word count program and compare the performance between MapReduce and Spark
2. Take a CSV file create hive table out of it, compare performance between SparkSQL and Hive queries

